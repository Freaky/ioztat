#!/usr/bin/env python3
# Based on https://www.reddit.com/r/zfs/comments/s0gxp0/ok_i_made_it_tool_to_show_io_for_individual/

# BSD 2-Clause License
#
# Copyright (c) 2022, Openoid LLC, on behalf of the r/zfs community
# All rights reserved.
#
# Redistribution and use in source and binary forms, with or without
# modification, are permitted provided that the following conditions are met:
#
# 1. Redistributions of source code must retain the above copyright notice, this
#    list of conditions and the following disclaimer.
#
# 2. Redistributions in binary form must reproduce the above copyright notice,
#    this list of conditions and the following disclaimer in the documentation
#    and/or other materials provided with the distribution.
#
# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS "AS IS"
# AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE
# IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE
# DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE
# FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL
# DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR
# SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER
# CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,
# OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE
# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.

PROGRAM_VERSION = '1.1.0-dev'

# In -p mode, indent each segment by this string
PATH_INDENT = '  '

import argparse
import os
import re
import shutil
import sys
import time

class Dataset:
    timestamp = 0
    name = ''
    writes = 0
    nwritten = 0
    reads = 0
    nread = 0

    def __init__(self, data = None):
        if data:
            self.timestamp = int(data['timestamp'])
            self.name = data['dataset_name']
            self.writes = int(data['writes'])
            self.nwritten = int(data['nwritten'])
            self.reads = int(data['reads'])
            self.nread = int(data['nread'])

if sys.platform.startswith("linux"):
    import glob

    def parseDatasets(pool = None):
        datasets = {}
        search = glob.escape(pool) if pool else '*'
        for file in glob.glob(os.path.join('/proc/spl/kstat/zfs', search, 'objset-*')):
            # Shortened example of these files:
            #############################################
            # 31 1 0x01 7 2160 6165792836 1634992995579
            # name             type data
            # dataset_name     7    rpool/ROOT/default
            #############################################
            # Field 7 of the header is a nanosecond data snapshot timestamp.
            # Conveniently, dataset names may not contain spaces.
            with open(file, 'r') as f:
                header, _fieldnames, *fields = [line.split() for line in f]
                fields.append(("timestamp", None, header[6]))
                ds = Dataset({field[0]: field[2] for field in fields if len(field) > 2})
                datasets[ds.name] = ds
        return datasets

elif sys.platform.startswith("freebsd"):
    try:
        # Attempt to use py-sysctl if available for best performance
        import sysctl
    except ImportError:
        # Otherwise make a simple sysctl(8) polyfill hopefully sufficient for our needs
        import subprocess

        class SysctlOid:
            def __init__(self, name = '', value = None):
                self.name = name
                self.value = value

        class sysctl:
            def filter(oid):
                r = subprocess.run(["sysctl", "-e", oid], capture_output=True, check=True, text=True)
                stats = [line.split("=", 2) for line in r.stdout.split("\n")]
                return [SysctlOid(*nv) for nv in stats if len(nv) == 2]

    from collections import defaultdict

    def parseDatasets(pool = None):
        # We're interested in kstat.zfs.*.dataset.objset-*.*
        # Note objset ID's are only unique to individual pools
        oid = "kstat.zfs."
        if pool:
            oid += pool + ".dataset."

        timestamp = time.monotonic_ns()
        ds = defaultdict(lambda: { 'timestamp': timestamp })
        for ctl in sysctl.filter(oid):
            name = ctl.name.rsplit(".", 4)
            if len(name) == 5 and name[2] == 'dataset':
                _, pool, _, objset, oid = name
                ds[(pool, objset)][oid] = ctl.value

        return {dataset.name: dataset for dataset in map(Dataset, ds.values())}

else:
    print("Unsupported platform: " + sys.platform)
    exit(1)

class DatasetDiff:
    def __init__(self, old, new):
        self.name = new.name
        self.timediff = (new.timestamp - old.timestamp) / 1e9
        self.wps = (new.writes - old.writes) / self.timediff
        self.wMBps = (new.nwritten - old.nwritten) / self.timediff
        self.rps = (new.reads - old.reads) / self.timediff
        self.rMBps = (new.nread - old.nread) / self.timediff

        try:
            self.rareq_sz = (new.nread - old.nread) / (new.reads - old.reads)
        except ZeroDivisionError:
            self.rareq_sz = 0

        try:
            self.wareq_sz = (new.nwritten - old.nwritten) / (new.writes - old.writes)
        except ZeroDivisionError:
            self.wareq_sz = 0

    def nonzero(self):
        """True if this DatasetDiff has any non-zero deltas"""
        return self.wps or self.wMBps or self.rps or self.rMBps

def calcDiff(prevdatasets, datasets):
    return [DatasetDiff(prevdatasets[key], datasets[key]) for key in datasets.keys() & prevdatasets.keys()]

def calcDiffFromStart(datasets):
    zero = Dataset()
    return [DatasetDiff(zero, dataset) for dataset in datasets.values()]

def dataset_iter(pools):
    """Iterate over name: Dataset for pools"""
    if pools:
        while True:
            yield {name: dataset for pool in pools for name, dataset in parseDatasets(pool).items()}
    else:
        while True:
            yield parseDatasets()

def diff_iter(pools):
    """Iterate over [DatasetDiff] for pools"""
    it = dataset_iter(pools)
    prevdatasets = next(it)
    # yield the initial summary
    yield calcDiffFromStart(prevdatasets)

    for datasets in it:
        yield calcDiff(prevdatasets, datasets)
        prevdatasets = datasets

def filter_diff_iter(pools, pattern, nonzero):
    """Iterate over [DatasetDiff] for pools filtered by a name pattern and their d.nonzero() state"""
    for diff in diff_iter(pools):
        yield list(filter(lambda d: pattern.match(d.name) and (not nonzero or d.nonzero()), diff))

def delay_iter(it, interval):
    """Add a delay after each iteration of a provided iterator"""
    for item in it:
        yield item
        time.sleep(interval)

# A more complete take on this is available here
# https://stackoverflow.com/questions/12523586/python-format-size-application-converting-b-to-kb-mb-gb-tb/63839503#63839503
SIZE_PREFIX = ['K', 'M', 'G', 'T', 'P', 'E']
SIZE_PREFIX_MAX = len(SIZE_PREFIX) - 1
def bytes_to_human(num, binary=False):
    div = 1024 if binary else 1000
    step = div - 0.05

    if num < div:
        return '{:.0f}'.format(num)

    num /= div

    for p in range(len(SIZE_PREFIX)):
        if num < step:
            break
        if p < SIZE_PREFIX_MAX:
            num /= div

    return '{:.1f}{}'.format(num, SIZE_PREFIX[p])

def format_dataset(path, limit, ellipsis = '...'):
    """Truncate a dataset name to a limit"""
    if len(path) <= limit:
        return path

    # Always display the pool name
    components = path.split('/', 1)
    ret = components[0]
    if len(components) > 1:
        ret += '/' + ellipsis + components[1][-(limit - (len(ret) + len(ellipsis) + 1)):]
    return ret

class Column:
    def __init__(self, name, width=0, just=str):
        self.name = name
        self.width = width
        self.just = just

    def format(self, string):
        if self.just:
            return self.just(string, self.width)
        else:
            return string

class ColumnGroup:
    def __init__(self, name, columns, width=0, just=str.center):
        self.name = name
        self.columns = columns
        self.width = width
        self.just = just

    def format(self, string):
        if self.just:
            return self.just(string, self.width + sum([c.width for c in self.columns]))
        else:
            return string

class ColumnFormatter:
    def __init__(self, column_separator = '  ', row_separator = '-'):
        self.columns = []
        self.groups = None
        self.column_separator = column_separator
        self.row_separator = row_separator

    def add_column(self, name, **args):
        self.columns.append(Column(name, **args))

    def add_group(self, name='', colspan=1, just=str.center):
        if self.groups is None:
            self.groups = ColumnGroupFormatter(column_separator=column_separator)

        self.groups.add_column(name, columns=self.columns[-colspan:], width=len(self.column_separator) * (colspan - 1), just=just)

    def set_width(self, column, width):
        self.columns[column].width = width

    def print_columns(self, columns):
        formatted = [c.format(str(column)) for (c, column) in zip(self.columns, columns)]
        print(self.column_separator.join(formatted))

    def print_header(self):
        if self.groups:
            self.groups.print_header()

        self.print_columns([c.name for c in self.columns])

    def print_divider(self):
        print(self.column_separator.join([self.row_separator * c.width for c in self.columns]))

class ColumnGroupFormatter(ColumnFormatter):
    def add_column(self, name, **args):
        self.columns.append(ColumnGroup(name, **args))

sorts = {
    'name':  {'key': lambda x: x.name},
    'rps':   {'key': lambda x: x.rps,   'reverse': True},
    'wps':   {'key': lambda x: x.wps,   'reverse': True},
    'rMBps': {'key': lambda x: x.rMBps, 'reverse': True},
    'wMBps': {'key': lambda x: x.wMBps, 'reverse': True},
}

# Keep this ordered alphabetically
parser = argparse.ArgumentParser(description='iostat for ZFS datasets', add_help=False)
parser.add_argument('dataset', type=str, nargs='*', help='ZFS dataset')
parser.add_argument('-b', dest='binaryprefix', default=False, action='store_true', help='use binary (power-of-two) prefixes')
parser.add_argument('-c', dest='count', type=int, help='number of reports generated')
parser.add_argument('-e', dest='exact', default=False, action='store_true', help='show exact values without truncation or scaling')
parser.add_argument('-H', dest='scripted', default=False, action='store_true', help='scripted mode, skip headers and tab-separate')
parser.add_argument('-h', '--help', action='help', help='show this help message and exit')
parser.add_argument('-i', dest='interval', default=1, type=float, help='interval between reports (in seconds)')
parser.add_argument('-n', dest='nonrecursive', default=False, action='store_true', help='do not recurse into child datasets')
parser.add_argument('-o', dest='overwrite', default=False, action='store_true', help='overwrite old reports in terminal')
group = parser.add_mutually_exclusive_group()
group.add_argument('-P', dest='fullname', default=None, action='store_true', help='display dataset names on a single line')
group.add_argument('-p', dest='fullname', default=None, action='store_false', help='display dataset names as an abbreviated tree')
parser.add_argument('-s', dest='sort', default='name', choices=sorts.keys(), help='sort by the specified field')
parser.add_argument('-T', dest='timestamp', default=False, choices=['u', 'd'], help='prefix each report with a Unix timestamp or formatted date')
parser.add_argument('-V', '--version', action='version', version='%(prog)s ' + PROGRAM_VERSION)
parser.add_argument('-y', dest='skip', default=0, action='store_const', const=1, help='skip the initial "summary" report')
parser.add_argument('-z', dest='nonzero', default=False, action='store_true', help='suppress datasets with zero activity')

args = parser.parse_args()

format_bytes = lambda num: bytes_to_human(num, args.binaryprefix)

# Enable full paths if we're not sorting by name or in nonzero mode, unless otherwise specified
if args.fullname is None:
    args.fullname = args.sort != 'name' or args.scripted or args.nonzero

pools = {dataset.split('/')[0] for dataset in args.dataset}
if args.nonrecursive:
    # Make each match exact.
    dataset_pattern = re.compile("|".join(map(lambda ds: "\A" + re.escape(ds) + "\Z", sorted(args.dataset))))
else:
    # Accept either an exact match or one with an additional component
    dataset_pattern = re.compile("|".join(map(lambda ds: "\A" + re.escape(ds) + "(?:\Z|/[^/]+)", sorted(args.dataset))))

# Iterate over the full list to get a dataset column width
if args.fullname:
    def calc_name_width(path):
        return len(path)
else:
    def calc_name_width(path):
        segments = path.split('/')
        return (len(segments[0:-1]) * len(PATH_INDENT)) + len(segments[-1])

summary = next(filter_diff_iter(pools, dataset_pattern, False))
max_name_width = max(map(lambda d: calc_name_width(d.name), summary), default=30)

diff_loop = filter_diff_iter(pools, dataset_pattern, args.nonzero)
diff_loop = delay_iter(diff_loop, args.interval)

if args.skip or args.count is not None:
    import itertools
    if args.count:
        args.count += args.skip
    diff_loop = itertools.islice(diff_loop, args.skip, args.count)

if not args.overwrite:
    diff_loop = filter(None, diff_loop)

NUM_NUMERIC_COLUMNS = 6

if args.scripted:
    column_separator = "\t"
    name_just = None
    num_just = None
    field_width = 0
else:
    column_separator = '  '
    name_just = str.ljust
    num_just = str.rjust
    field_width = 11 if args.exact else 5

formatter = ColumnFormatter(column_separator = column_separator, row_separator = '-')
formatter.add_column('dataset', just=name_just)
formatter.add_group()
formatter.add_column('read',  width=field_width, just=num_just)
formatter.add_column('write', width=field_width, just=num_just)
formatter.add_group('operations', 2)
formatter.add_column('read',  width=field_width, just=num_just)
formatter.add_column('write', width=field_width, just=num_just)
formatter.add_group('bandwidth', 2)
formatter.add_column('read',  width=field_width, just=num_just)
formatter.add_column('write', width=field_width, just=num_just)
formatter.add_group('opsize', 2)

try:
    for index, diff in enumerate(diff_loop):
        # Always sort by name first, so the main sort has it as a secondary
        diff.sort(**sorts['name'])
        if args.sort != 'name':
            diff.sort(**sorts[args.sort])

        # TODO: repeat headers based on height when connected to a tty
        width, _ = shutil.get_terminal_size()
        avail_width = width - ((field_width + len(column_separator)) * NUM_NUMERIC_COLUMNS)
        name_width = max(10, min([avail_width, max_name_width]))
        formatter.columns[0].width = name_width
        # formatter.groups.columns[0].width = name_width

        if args.overwrite:
            # Clear the screen and move the cursor to the upper-left
            print("\033[2J\033[1;1H", end = '')

        if args.timestamp == 'u':
            print(int(time.time()))
        elif args.timestamp == 'd':
            print(time.strftime('%c'))

        if (index == 0 or args.overwrite) and not args.scripted:
            formatter.print_header()
            formatter.print_divider()

        last_path = []
        for d in diff:
            max_name_width = max(calc_name_width(d.name), max_name_width)
            if args.fullname:
                name = d.name
            else:
                cur_path = d.name.split('/')
                # Unmounted intermediate datasets can leave confusing gaps.
                # Find the longest shared segment with the previous path,
                # and print any missing intermediates.
                common = os.path.commonprefix([last_path, cur_path])
                for i, segment in enumerate(cur_path[len(common):-1]):
                    print((PATH_INDENT * (len(common) + i)) + segment)
                name = (PATH_INDENT * (len(cur_path) - 1)) + cur_path[-1]
                last_path = cur_path

            if args.exact:
                cols = (name, round(d.rps), round(d.wps), round(d.rMBps), round(d.wMBps), round(d.rareq_sz), round(d.wareq_sz))
            else:
                cols = (
                    format_dataset(name, name_width), round(d.rps), round(d.wps),
                    format_bytes(d.rMBps), format_bytes(d.wMBps),
                    format_bytes(d.rareq_sz), format_bytes(d.wareq_sz)
                )

            formatter.print_columns(cols)

        if not (args.scripted or args.overwrite):
            formatter.print_divider()

except KeyboardInterrupt:
    print('')
